#!/usr/bin/env python3
"""
pipeline_runner.py
==================
Tek bir komutla **uÃ§tan uca** raporâ€iÅŸleme ve cevap Ã¼retme sÃ¼recini yÃ¶netir.

AÅŸamalar
--------
1. Workspace klasÃ¶rlerini hazÄ±rla (`init_workspace`)
2. PDF â†’ TXT (`pdf_to_text`)
3. CID temizliÄŸi (`cid_cleaner`)
4. Chunk oluÅŸturma (`chunk_creator`)
5. Chunk embed + FAISS (`faiss_creator`)
6. Soruâ€‘yordam embed + FAISS (`soru_yordam_embedder`)
7. Her soru iÃ§in topâ€‘k chunk bul (`search_faiss_top_chunks`)
8. Chunkâ€™larÄ± geniÅŸlet (`expand_top10_chunks`)
9. Prompt Ã¼ret (`gpt_prompt_builder`)
10. (Opsiyonel) GPTâ€™ye gÃ¶nder, cevaplarÄ± kaydet (`sender`)

Ortam DeÄŸiÅŸkenleri (.env)
-------------------------
OPENAI_API_KEY, WORKSPACE_ROOT, EMBED_MODEL, TOPK vb. deÄŸerler otomatik
okunur (pythonâ€‘dotenv).
"""

from __future__ import annotations

import os
import uuid
import argparse
from pathlib import Path
from dotenv import load_dotenv

# âžŠ  Pipeline adÄ±mlarÄ±nÄ± iÃ§e aktar
from init_workspace import init_workspace
from pdf_to_text import pdf_to_txt
from cid_cleaner import clean_txt
from chunk_creator import create_chunks
from faiss_creator import create_faiss_for_chunks
from soru_yordam_embedder import vectorize_soru_yordam
from search_faiss_top_chunks import ask_all
from expand_top10_chunks import expand_chunk
from gpt_prompt_builder import generate_all_prompts
from sender import send_answers

# --------------------------------------------------
#  Ortam deÄŸiÅŸkenlerini yÃ¼kle (.env dosyasÄ±)
# --------------------------------------------------

load_dotenv()  # proje kÃ¶kÃ¼ndeki .env okunur

# --------------------------------------------------
#  Ana Ã§alÄ±ÅŸma fonksiyonu
# --------------------------------------------------

def run_pipeline(
    *,
    pdf_path: str | Path,
    questions_path: str | Path,
    report_id: str | None = None,
    send_to_gpt: bool = True,
    embed_model: str | None = None,
    top_k: int | None = None,
) -> Path:
    """TÃ¼m adÄ±mlarÄ± sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±r ve workspace yolunu dÃ¶ndÃ¼rÃ¼r."""

    # ---- Ayarlar (.env + parametre) ----------------
    workspace_root = Path(os.getenv("WORKSPACE_ROOT", "workspace")).expanduser()
    embed_model = embed_model or os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
    top_k = top_k or int(os.getenv("TOPK", "10"))

    # ---- Workspace -------------------------------
    report_id = report_id or Path(pdf_path).stem or f"r_{uuid.uuid4().hex[:6]}"
    workspace_dir = workspace_root / report_id
    workspace_dir.mkdir(parents=True, exist_ok=True)

    # 1. klasÃ¶r yapÄ±sÄ±
    init_workspace(report_id, str(workspace_root))

    # 2. PDF â†’ TXT
    txt_path = pdf_to_txt(str(pdf_path), str(workspace_dir))

    # 3. CID fix
    clean_path = clean_txt(txt_path, str(workspace_dir))

    # 4. Chunk oluÅŸtur
    create_chunks(clean_path, str(workspace_dir))

    # 5. Chunk embed â†’ FAISS
    create_faiss_for_chunks(str(workspace_dir), embed_model)

    # 6. Soruâ€‘yordam embed â†’ FAISS
    vectorize_soru_yordam(str(questions_path), str(workspace_dir), embed_model)

    # 7. Topâ€‘k chunk bul
    ask_all(str(workspace_dir), top_k=top_k, model_name=embed_model)

    # 8. Chunk geniÅŸlet
    expand_chunk(str(workspace_dir))

    # 9. Prompt Ã¼ret
    generate_all_prompts(workspace_dir)

    # 10. Cevap al (isteÄŸe baÄŸlÄ±)
    #if send_to_gpt:
        #send_answers(workspace_dir)

    print("ðŸŽ‰ Pipeline tamamlandÄ± â†’", workspace_dir)
    return workspace_dir

# --------------------------------------------------
#  CLI sarÄ±cÄ±
# --------------------------------------------------

def _cli() -> None:
    p = argparse.ArgumentParser(description="Tek komutla rapor pipeline'Ä± Ã§alÄ±ÅŸtÄ±r.")
    p.add_argument("pdf", help="Kaynak PDF dosyasÄ±")
    p.add_argument("questions", help="Soruâ€‘yordam .txt veya .json dosyasÄ±")
    p.add_argument("--id", dest="report_id", default=None, help="Rapor kimliÄŸi (klasÃ¶r adÄ±)")
    p.add_argument("--no-gpt", action="store_true", help="GPT'ye gÃ¶ndermeden dur")
    p.add_argument("--model", dest="embed_model", default=None, help="Sentenceâ€‘Transformers modeli")
    p.add_argument("--topk", dest="top_k", type=int, default=None, help="Topâ€‘k chunk sayÄ±sÄ±")
    args = p.parse_args()

    run_pipeline(
        pdf_path=args.pdf,
        questions_path=args.questions,
        report_id=args.report_id,
        send_to_gpt=not args.no_gpt,
        embed_model=args.embed_model,
        top_k=args.top_k,
    )

if __name__ == "__main__":  # pragma: no cover
    _cli()


'''
How to use this script:
from pdf_to_text import pdf_to_txt
pdf_to_txt("user_uploads/rapor2024.pdf", "workspace/rapor2024")

'''